# -*- coding: utf-8 -*-
"""StressCauseIdentificationLSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jeco20DfM7oZv4G4RskyJNp2a4i7HjZ5
"""

from google.colab import drive
drive.mount('/content/drive/')

!pip install -q keras

import keras

import numpy as np # linear algebra
import pandas as pd # data processing
import tensorflow as tf
from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D
from tensorflow.keras.models import Sequential
from sklearn.feature_extraction.text import CountVectorizer
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
print(tf.__version__)

data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Dataset/TestforNLP3.csv')

data.head()

data.Subreddit.value_counts()  #Displaying total number of values for each subreddit (class)

data.Text=data.Text.astype(str)   #Converting to string

#Integer encoding
data.loc[data['Subreddit'] == 'Divorce', 'LABEL'] = 0
data.loc[data['Subreddit'] == 'CPTSD', 'LABEL'] = 1
data.loc[data['Subreddit'] == 'Anxiety', 'LABEL'] = 2
data.loc[data['Subreddit'] == 'Suicide', 'LABEL'] = 3
data.loc[data['Subreddit'] == 'Depression', 'LABEL'] = 4
data.loc[data['Subreddit'] == 'Breakups', 'LABEL'] = 5
print(data['LABEL'][:10])
data.head()

data['LABEL']

labels = to_categorical(data['LABEL'], num_classes=6)
print(labels[:10])
if 'Subreddit' in data.keys():
    data.drop(['Subreddit'], axis=1)

data.head()

#preprocessing
n_most_common_words = 8000
max_len = 130
tokenizer = Tokenizer(num_words=n_most_common_words, filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~')
tokenizer.fit_on_texts(data['Text'].values)
sequences = tokenizer.texts_to_sequences(data['Text'].values)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

X = pad_sequences(sequences, maxlen=max_len)

X

X = X.astype('float32')  #Converting to Float

X

X_train, X_test, y_train, y_test = train_test_split(X , labels, test_size=0.25, random_state=42)  #Spliting for training and validation

epochs = 50
emb_dim = 128
batch_size = 256
labels[:2]

print((X_train.shape, y_train.shape, X_test.shape, y_test.shape))
 #creating the model
def create_model():
    model = Sequential()
    model.add(Embedding(n_most_common_words, emb_dim, input_length=X.shape[1]))
    model.add(SpatialDropout1D(0.5))
    model.add(LSTM(64, dropout=0.5, recurrent_dropout=0.5))
    model.add(Dense(6, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
    print(model.summary())
    return model

model = create_model()

from keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=8)  #earlystopping to prevent overfitting
model = create_model()

history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=[early_stopping])
model.save('model.h5')  #saving model for later use

#obtaining model input and output tensor names
print("model inputs : " + str(model.inputs))
print("model outputs : " + str(model.outputs))

#plotting accuracy and loss for model
import matplotlib.pyplot as plt

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

from keras import backend as K
from keras.models import load_model

#converting model to tflite for Android Studio use
tflite_model = tf.keras.models.load_model('model.h5')
converter = tf.lite.TFLiteConverter.from_keras_model(tflite_model)
tflite_save = converter.convert()
open("stress_classification_model.tflite", "wb").write(tflite_save)

#testing
txt = ["I absolutely HATE seeing videos of children posted online with the intent of humiliating them"]
seq = tokenizer.texts_to_sequences(txt)
padded = pad_sequences(seq, maxlen=max_len)
pred = model.predict(padded)
labels = ['Divorce', 'CPTSD', 'Anxiety', 'Suicide', 'Depression', 'Breakups']
print(pred, labels[np.argmax(pred)])

txt2 = ["everyday my crippling anxiety gets worse, i can’t get out of bed, i’m always severely depressed, whenever i even think about leaving my house i get extremely sick and completely lose control of my emotions. i’ve quit my job because even thinking about it causes a panic attack. my anxiety is destroying my life and i’m losing faith in myself. i don’t want to live like this for the rest of my life and the fact that i have no control over my own life is depressing in itself. if anyone has any tips or answers please let me know. i’m desperate"]
seq = tokenizer.texts_to_sequences(txt2)
padded = pad_sequences(seq, maxlen=max_len)
pred = model.predict(padded)
labels = ['Divorce', 'CPTSD', 'Anxiety', 'Suicide', 'Depression', 'Breakups']
print(pred, labels[np.argmax(pred)])

txt3 = ['Im a 21 year old college dropout working a customer service job with 16 year olds who are way smarter than me. I have no prospects of ever moving up in the world. I wont be able to go back to college probably ever, and I dont know and cant do shit anymore. Im just another meaningless high school diploma idiot in the eyes of society and myself.']
seq = tokenizer.texts_to_sequences(txt3)
padded = pad_sequences(seq, maxlen=max_len)
pred = model.predict(padded)
labels = ['Divorce', 'CPTSD', 'Anxiety', 'Suicide', 'Depression', 'Breakups']
print(pred, labels[np.argmax(pred)])

txt4 = ['I have finally accepted the fact that my relationship with my ex is over. It doesnt matter anymore. Moving on. This is my last post on here.']
seq = tokenizer.texts_to_sequences(txt4)
padded = pad_sequences(seq, maxlen=max_len)
pred = model.predict(padded)
labels = ['Divorce', 'CPTSD', 'Anxiety', 'Suicide', 'Depression', 'Breakups']
print(pred, labels[np.argmax(pred)])

from google.colab import drive
drive.mount('/content/drive')

txt5 = ['I still remembered clearly that day. I was alone in my room begging if somebody could hold me back, even with a simple word "dont die", "please stay", but that did not happen. I took all the pills and woke up in the hospital 2 days later. The thing I regret the most is not attempting suicide on that day, but its the pain of having to deal with everything after that. The explanation, the getting better thing, and its been more than a year, still Im taken back to that room occasionally, to remind me of all the pain that push me to do that. I just want to die, selfishly.']
seq = tokenizer.texts_to_sequences(txt5)
padded = pad_sequences(seq, maxlen=max_len)
pred = model_load.predict(padded)
labels = ['Divorce', 'CPTSD', 'Anxiety', 'Suicide', 'Depression', 'Breakups']
print(pred, labels[np.argmax(pred)])

from tensorflow import keras
model_load = keras.models.load_model('/content/model.h5')
model_load.summary()

#testing with loaded model
txt5 = ['I still remembered clearly that day. I was alone in my room begging if somebody could hold me back, even with a simple word "dont die", "please stay", but that did not happen. I took all the pills and woke up in the hospital 2 days later. The thing I regret the most is not attempting suicide on that day, but its the pain of having to deal with everything after that. The explanation, the getting better thing, and its been more than a year, still Im taken back to that room occasionally, to remind me of all the pain that push me to do that. I just want to die, selfishly.']
seq = tokenizer.texts_to_sequences(txt5)
padded = pad_sequences(seq, maxlen=max_len)
pred = model_load.predict(padded)
labels = ['Divorce', 'CPTSD', 'Anxiety', 'Suicide', 'Depression', 'Breakups']
print(pred, labels[np.argmax(pred)])

import json
 #saving word dictionary as json file
with open( 'words_dict.txt' , 'w' ) as file:
	json.dump( tokenizer.word_index , file )

txt6 = ['please help me i am so depressed']
seq = tokenizer.texts_to_sequences(txt6)
padded = pad_sequences(seq, maxlen=max_len)
pred = model_load.predict(padded)
labels = ['Divorce', 'CPTSD', 'Anxiety', 'Suicide', 'Depression', 'Breakups']
print(pred, labels[np.argmax(pred)])

tflite_model = 'stress_classification_model.tflite'

interpreter = tf.lite.Interpreter(model_path=tflite_model)
interpreter.allocate_tensors()

txt7 = ['please help me i am so depressed']
seq = tokenizer.texts_to_sequences(txt7)
paddeds = pad_sequences(seq, maxlen=max_len)

input_details = interpreter.get_input_details()
output_details =interpreter.get_output_details()

input_shape = input_details[0]['shape']
input_data = np.array(paddeds, dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()

output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)

labels =['Divorce', 'CPTSD', 'Anxiety', 'Suicide', 'Depression', 'Breakups']
print(output_data, labels[np.argmax(output_data)])

input_details

paddeds

seq

paddeds = [[403.0, 114.0, 8.0, 1.0, 54.0, 21.0, 365.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]

input_details = interpreter.get_input_details()
output_details =interpreter.get_output_details()

input_shape = input_details[0]['shape']
input_data = np.array(paddeds, dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()

output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)

labels =['Divorce', 'CPTSD', 'Anxiety', 'Suicide', 'Depression', 'Breakups']
print(output_data, labels[np.argmax(output_data)])

